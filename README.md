SVM â€” Binary Classification with 2 Features ðŸ§ª
This notebook compares Support Vector Machine (SVM) kernelsâ€”linear, rbf, poly, sigmoidâ€”on a 2D subset of the Breast Cancer dataset to classify tumors as benign (0) or malignant (1).

Goal: Build a compact ML workflow (EDA â†’ simple preprocessing â†’ training â†’ evaluation) and see how different SVM kernels behave in a low-dimensional setting.

ðŸ”¹ Dataset & Features

Source columns: radius_mean, texture_mean, diagnosis

Label mapping: M â†’ 1, B â†’ 0

Train/Test split: 75% / 25%, random_state=15

ðŸ”¹ What We Did

EDA & Prep: Quick structure check; select 2 features; map labels

Visualization: 2D scatter plot colored by class

Models: SVC with linear, rbf, poly, sigmoid

Metrics: classification_report + confusion_matrix

Tuning: GridSearchCV on RBF (C, gamma) to find best hyperparameters

ðŸ”¹ Key Results (Test Set)

Linear SVM: ~0.85 accuracy (balanced precision/recall)

RBF SVM: ~0.87 accuracy (strong baseline)

Poly / Sigmoid: ~0.63 accuracy (collapsed to majority class)

Best RBF via GridSearch: C=10, gamma=0.01, ~0.86 accuracy with improved balance

ðŸ”¹ Takeaways

With only two informative features, linear/RBF kernels perform reliably.

Poly/Sigmoid can underperform or collapse without careful tuning/feature scaling.

Simple 2D setup makes decision boundaries easy to interpret while demonstrating kernel effects.

ðŸ”¹ Tech Stack

Python â€¢ pandas â€¢ numpy â€¢ scikit-learn â€¢ seaborn â€¢ matplotlib
